{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud detection with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Suppress warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data processing, metrics and modeling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('newtrain_transaction_200000.csv', index_col='TransactionID')\n",
    "test = pd.read_csv('newtest_transaction_withoutlabel.csv', index_col='TransactionID')\n",
    "train_identity = pd.read_csv('newtrain_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('newtest_identity.csv', index_col='TransactionID')\n",
    "pred_df = pd.read_csv('newsample_submission.csv', index_col='TransactionID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data['isFraud'].copy()\n",
    "data=data.drop(['isFraud'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80397, 392)\n",
      "(10000, 392)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preserve Transaction AMT as weight\n",
    "# import torch\n",
    "# weight=data['TransactionAmt'].copy()\n",
    "# weight=weight/100\n",
    "# print(weight.max())\n",
    "# print(weight.min())\n",
    "# weight=weight.to_list()\n",
    "# weight=torch.Tensor(weight)\n",
    "# print(type(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 53)\n",
      "(80397, 53)\n"
     ]
    }
   ],
   "source": [
    "#Drop all the V***\n",
    "test = test.iloc[:, :53]\n",
    "print(test.shape)\n",
    "data = data.iloc[:, :53]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80397, 38)\n",
      "(10000, 38)\n"
     ]
    }
   ],
   "source": [
    "#Drop D1 to D15\n",
    "drop_list=['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14','D15']\n",
    "for col in drop_list:\n",
    "    data=data.drop([col],axis=1)\n",
    "    test=test.drop([col],axis=1)\n",
    "print(data.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80397, 59)\n",
      "(10000, 59)\n"
     ]
    }
   ],
   "source": [
    "#Drop identity columns with high level NANs\n",
    "train_NA = train_identity.isna().sum(axis=0)\n",
    "test_NA = test_identity.isna().sum(axis=0)\n",
    "for indexs in train_NA.index:\n",
    "    if train_NA[indexs]>40000: \n",
    "        train_identity=train_identity.drop([indexs],axis=1)\n",
    "        test_identity=test_identity.drop([indexs],axis=1)\n",
    "# print(train_NA)\n",
    "data = data.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "print(data.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical and Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [\n",
    "    'ProductCD',\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2',\n",
    "    'P_emaildomain',\n",
    "    'R_emaildomain',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'DeviceType','DeviceInfo',\n",
    "    'id_12','id_13','id_14','id_15','id_16','id_17','id_18','id_19',\n",
    "    'id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29',\n",
    "    'id_30','id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38'\n",
    "]\n",
    "\n",
    "categorical = [col for col in categorical if col in data.columns]\n",
    "numerical=[]\n",
    "for col in data.columns:\n",
    "    if not(col in categorical): numerical.append(col)\n",
    "# print(numerical)\n",
    "# print(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical scaling\n",
    "For continuous right-skewed features we wil apply log-transform, so that will make them look more like normal distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ContinuousFeatureConverter:\n",
    "#     def __init__(self, name, feature, log_transform):\n",
    "#         self.name = name\n",
    "#         self.skew = feature.skew()\n",
    "#         self.log_transform = log_transform\n",
    "        \n",
    "#     def transform(self, feature):\n",
    "#         if self.skew > 1:\n",
    "#             feature = self.log_transform(feature)\n",
    "        \n",
    "#         mean = feature.mean()\n",
    "#         std = feature.std()\n",
    "#         return (feature - mean)/(std + 1e-6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "for column in numerical:\n",
    "#     print(data[column])\n",
    "    scaler = StandardScaler()\n",
    "    if data[column].max() > 100 and data[column].min() >= 0:\n",
    "        data[column] = np.log1p(data[column])\n",
    "        test[column] = np.log1p(test[column])\n",
    "    scaler.fit(np.concatenate([data[column].values.reshape(-1,1), test[column].values.reshape(-1,1)]))\n",
    "    data[column] = scaler.transform(data[column].values.reshape(-1,1))\n",
    "    test[column] = scaler.transform(test[column].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill nan (with 0: not the optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan2mean(df):\n",
    "    for x in list(df.columns.values):\n",
    "        if x in numerical:\n",
    "            df[x] = df[x].fillna(0)\n",
    "    return df\n",
    "\n",
    "data=nan2mean(data)\n",
    "test=nan2mean(test)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "category_counts = {}\n",
    "for f in categorical:\n",
    "    data[f] = data[f].replace(\"nan\", \"other\")\n",
    "    data[f] = data[f].replace(np.nan, \"other\")\n",
    "    test[f] = test[f].replace(\"nan\", \"other\")\n",
    "    test[f] = test[f].replace(np.nan, \"other\")\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(data[f].values) + list(test[f].values))\n",
    "    data[f] = lbl.transform(list(data[f].values))\n",
    "    test[f] = lbl.transform(list(test[f].values))\n",
    "    category_counts[f] = len(list(lbl.classes_)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(data,labels,test_size=0.2, random_state=13)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras import regularizers\n",
    "\n",
    "network=models.Sequential()\n",
    "network.add(layers.Dense(200, activation='relu', input_shape=(59,)))\n",
    "# network.add(layers.Dropout(0.1))\n",
    "network.add(layers.Dense(100, activation='relu'))\n",
    "network.add(layers.Dense(60, activation='relu'))\n",
    "# network.add(layers.Dropout(0.1))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.1101 - accuracy: 0.9751\n",
      "Epoch 2/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0796 - accuracy: 0.9791\n",
      "Epoch 3/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0755 - accuracy: 0.9799\n",
      "Epoch 4/63\n",
      "8040/8040 [==============================] - 11s 1ms/step - loss: 0.0742 - accuracy: 0.9802\n",
      "Epoch 5/63\n",
      "8040/8040 [==============================] - 12s 1ms/step - loss: 0.0725 - accuracy: 0.9802\n",
      "Epoch 6/63\n",
      "8040/8040 [==============================] - 12s 1ms/step - loss: 0.0719 - accuracy: 0.9804\n",
      "Epoch 7/63\n",
      "8040/8040 [==============================] - 12s 2ms/step - loss: 0.0716 - accuracy: 0.9808\n",
      "Epoch 8/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0706 - accuracy: 0.9807\n",
      "Epoch 9/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0706 - accuracy: 0.9809\n",
      "Epoch 10/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0699 - accuracy: 0.9811\n",
      "Epoch 11/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0700 - accuracy: 0.9812\n",
      "Epoch 12/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0692 - accuracy: 0.9810\n",
      "Epoch 13/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0694 - accuracy: 0.9812\n",
      "Epoch 14/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0692 - accuracy: 0.9812\n",
      "Epoch 15/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0687 - accuracy: 0.9813\n",
      "Epoch 16/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0687 - accuracy: 0.9812\n",
      "Epoch 17/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0687 - accuracy: 0.9814\n",
      "Epoch 18/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0684 - accuracy: 0.9813\n",
      "Epoch 19/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0684 - accuracy: 0.9814\n",
      "Epoch 20/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0681 - accuracy: 0.9815\n",
      "Epoch 21/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0681 - accuracy: 0.9814\n",
      "Epoch 22/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0678 - accuracy: 0.9814\n",
      "Epoch 23/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0679 - accuracy: 0.9815\n",
      "Epoch 24/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0675 - accuracy: 0.9812\n",
      "Epoch 25/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0677 - accuracy: 0.9814\n",
      "Epoch 26/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0672 - accuracy: 0.9817\n",
      "Epoch 27/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0671 - accuracy: 0.9816\n",
      "Epoch 28/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0673 - accuracy: 0.9815 0s - loss: 0.0670 - accu\n",
      "Epoch 29/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0673 - accuracy: 0.9816 0s - loss: 0.0675 - accuracy: \n",
      "Epoch 30/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0673 - accuracy: 0.9813\n",
      "Epoch 31/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0672 - accuracy: 0.9815\n",
      "Epoch 32/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0671 - accuracy: 0.9816\n",
      "Epoch 33/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0669 - accuracy: 0.9817\n",
      "Epoch 34/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0669 - accuracy: 0.9814\n",
      "Epoch 35/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0668 - accuracy: 0.9819\n",
      "Epoch 36/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0667 - accuracy: 0.9816\n",
      "Epoch 37/63\n",
      "8040/8040 [==============================] - 13s 2ms/step - loss: 0.0667 - accuracy: 0.9816\n",
      "Epoch 38/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0667 - accuracy: 0.9815\n",
      "Epoch 39/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0666 - accuracy: 0.9818\n",
      "Epoch 40/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0665 - accuracy: 0.9815\n",
      "Epoch 41/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0663 - accuracy: 0.9820\n",
      "Epoch 42/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0663 - accuracy: 0.9816\n",
      "Epoch 43/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0664 - accuracy: 0.9816\n",
      "Epoch 44/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0664 - accuracy: 0.9819 0s - loss: 0\n",
      "Epoch 45/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0664 - accuracy: 0.9817\n",
      "Epoch 46/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0662 - accuracy: 0.9818 0s - loss: 0.0\n",
      "Epoch 47/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0662 - accuracy: 0.9818\n",
      "Epoch 48/63\n",
      "8040/8040 [==============================] - 18s 2ms/step - loss: 0.0662 - accuracy: 0.9818\n",
      "Epoch 49/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0661 - accuracy: 0.9819\n",
      "Epoch 50/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0660 - accuracy: 0.9818 0s - loss:\n",
      "Epoch 51/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0661 - accuracy: 0.9817\n",
      "Epoch 52/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0658 - accuracy: 0.9817\n",
      "Epoch 53/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0660 - accuracy: 0.9818\n",
      "Epoch 54/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0660 - accuracy: 0.9820\n",
      "Epoch 55/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0659 - accuracy: 0.9819\n",
      "Epoch 56/63\n",
      "8040/8040 [==============================] - 20s 3ms/step - loss: 0.0659 - accuracy: 0.9819\n",
      "Epoch 57/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0659 - accuracy: 0.9818\n",
      "Epoch 58/63\n",
      "8040/8040 [==============================] - 19s 2ms/step - loss: 0.0658 - accuracy: 0.9820-\n",
      "Epoch 59/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0657 - accuracy: 0.9821\n",
      "Epoch 60/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0658 - accuracy: 0.9819\n",
      "Epoch 61/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0656 - accuracy: 0.9820\n",
      "Epoch 62/63\n",
      "8040/8040 [==============================] - 20s 2ms/step - loss: 0.0656 - accuracy: 0.9820\n",
      "Epoch 63/63\n",
      "8040/8040 [==============================] - 20s 3ms/step - loss: 0.0655 - accuracy: 0.9819\n",
      "503/503 [==============================] - 1s 3ms/step - loss: 0.0964 - accuracy: 0.9778: 0s - loss: 0.1001 - accuracy\n",
      "Epoch 1/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.1217 - accuracy: 0.9735\n",
      "Epoch 2/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0811 - accuracy: 0.9791\n",
      "Epoch 3/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0770 - accuracy: 0.9796 1s - loss: 0 - ETA:  - ETA: 0s - loss: 0.0768 \n",
      "Epoch 4/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0745 - accuracy: 0.9803\n",
      "Epoch 5/63\n",
      "8040/8040 [==============================] - 16s 2ms/step - loss: 0.0729 - accuracy: 0.9804\n",
      "Epoch 6/63\n",
      "8040/8040 [==============================] - 21s 3ms/step - loss: 0.0722 - accuracy: 0.9810\n",
      "Epoch 7/63\n",
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0717 - accuracy: 0.9808 0s\n",
      "Epoch 8/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0706 - accuracy: 0.9809\n",
      "Epoch 9/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0703 - accuracy: 0.9807\n",
      "Epoch 10/63\n",
      "8040/8040 [==============================] - 19s 2ms/step - loss: 0.0699 - accuracy: 0.9810\n",
      "Epoch 11/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0696 - accuracy: 0.9810\n",
      "Epoch 12/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0692 - accuracy: 0.9811 0s - loss:\n",
      "Epoch 13/63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8040/8040 [==============================] - 17s 2ms/step - loss: 0.0689 - accuracy: 0.9814\n",
      "Epoch 14/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0686 - accuracy: 0.9811\n",
      "Epoch 15/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0687 - accuracy: 0.9812\n",
      "Epoch 16/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0684 - accuracy: 0.9814\n",
      "Epoch 17/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0682 - accuracy: 0.9815\n",
      "Epoch 18/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0680 - accuracy: 0.9812\n",
      "Epoch 19/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0680 - accuracy: 0.9816\n",
      "Epoch 20/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0675 - accuracy: 0.9814\n",
      "Epoch 21/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0677 - accuracy: 0.9817\n",
      "Epoch 22/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0673 - accuracy: 0.9815\n",
      "Epoch 23/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0673 - accuracy: 0.9815\n",
      "Epoch 24/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0670 - accuracy: 0.9819\n",
      "Epoch 25/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0670 - accuracy: 0.9817\n",
      "Epoch 26/63\n",
      "8040/8040 [==============================] - 14s 2ms/step - loss: 0.0672 - accuracy: 0.9816\n",
      "Epoch 27/63\n",
      "8040/8040 [==============================] - 15s 2ms/step - loss: 0.0668 - accuracy: 0.9815\n",
      "Epoch 28/63\n",
      "8040/8040 [==============================] - 20s 3ms/step - loss: 0.0669 - accuracy: 0.9817 0s - loss: 0\n",
      "Epoch 29/63\n",
      " 632/8040 [=>............................] - ETA: 21s - loss: 0.0571 - accuracy: 0.9836"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-868e9236546e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     network.compile(optimizer='Adagrad',loss='binary_crossentropy',\n\u001b[1;32m     17\u001b[0m             metrics=['accuracy'])\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m63\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Use binary crossentropy loss\n",
    "# network.compile(optimizer='rmsprop',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "#define a new loss function\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# def new_loss(y_true,y_pred):\n",
    "#     loss=weight*K.binary_crossentropy(y_pred, y_true)\n",
    "#     return K.mean(loss, axis=-1)\n",
    "\n",
    "loss=[]\n",
    "# for i in range(10,101,10):\n",
    "network.compile(optimizer='Adagrad',loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "network.fit(X_train, y_train, epochs=63, batch_size=128)\n",
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "loss.append(test_loss)\n",
    "    \n",
    "# x=range(10,101,10)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(x, loss)\n",
    "# plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503/503 [==============================] - 1s 1ms/step - loss: 0.0967 - accuracy: 0.9779\n",
      "test_acc: 0.9779228568077087\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss calculation for homework2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.9338\n",
      "0.9337999820709229\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "      TransactionID  Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
      "0           2987022           0             0               0   \n",
      "1           2987100           1             1               1   \n",
      "2           2987104           2             2               2   \n",
      "3           2987148           3             3               3   \n",
      "4           2987288           4             4               4   \n",
      "...             ...         ...           ...             ...   \n",
      "9995        3577009        9995          9995            9995   \n",
      "9996        3577027        9996          9996            9996   \n",
      "9997        3577070        9997          9997            9997   \n",
      "9998        3577183        9998          9998            9998   \n",
      "9999        3577465        9999          9999            9999   \n",
      "\n",
      "      Unnamed: 0.1.1.1  level_0  Unnamed: 0.1.1.1.1  index   isFraud  \n",
      "0                    0        0                   0      0  0.000001  \n",
      "1                    1        1                   1      1  0.002229  \n",
      "2                    2        2                   2      2  0.000657  \n",
      "3                    3        3                   3      3  0.002500  \n",
      "4                    4        4                   4      4  0.063043  \n",
      "...                ...      ...                 ...    ...       ...  \n",
      "9995              9995     9995                9995   9995  0.000586  \n",
      "9996              9996     9996                9996   9996  0.005003  \n",
      "9997              9997     9997                9997   9997  0.000003  \n",
      "9998              9998     9998                9998   9998  0.000876  \n",
      "9999              9999     9999                9999   9999  0.000101  \n",
      "\n",
      "[10000 rows x 9 columns]\n",
      "total fraud 805\n",
      "cutoff 0.04609236\n",
      "missed_fraud 366\n",
      "mean missed fraud amt 96.63725409836066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35369.235"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(df_pred, df_true):\n",
    "    fraud_set = set(df_true[df_true['isFraud']==1]['TransactionID'])\n",
    "    print('total fraud',len(fraud_set))\n",
    "    cutoff = df_pred['isFraud'].nlargest(1000).iloc[-1]\n",
    "    print('cutoff',cutoff)\n",
    "    pass_set = set(df_pred[df_pred['isFraud']<cutoff]['TransactionID'])\n",
    "    fraud_miss = fraud_set & pass_set\n",
    "    df_fraud_miss = df_true[df_true['TransactionID'].isin(fraud_miss)]\n",
    "    print('missed_fraud',df_fraud_miss.shape[0])\n",
    "    loss = df_fraud_miss['TransactionAmt'].sum()\n",
    "    print('mean missed fraud amt',loss/df_fraud_miss.shape[0])\n",
    "    return loss   \n",
    "\n",
    "answer=pd.read_csv('newtest_transaction.csv')\n",
    "test_answer=answer['isFraud'].copy()\n",
    "test_loss, test_acc = network.evaluate(test, test_answer)\n",
    "print(test_acc)\n",
    "\n",
    "pred=network.predict(test,batch_size = 2000, verbose = True)\n",
    "pred_df['isFraud']=pred\n",
    "pred_df=pred_df.reset_index()\n",
    "pred_df.to_csv('newsample_submission.csv')\n",
    "\n",
    "print(pred_df)\n",
    "# print(answer)\n",
    "compute_loss(pred_df,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
