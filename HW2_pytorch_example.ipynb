{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80397, 433)\n",
      "(10000, 433)\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_transaction = pd.read_csv('newtrain_transaction_200000.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('newtest_transaction.csv', index_col='TransactionID')\n",
    "\n",
    "train_identity = pd.read_csv('newtrain_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('newtest_identity.csv', index_col='TransactionID')\n",
    "\n",
    "sample_submission = pd.read_csv('newsample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "y_train = train['isFraud'].copy()\n",
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "\n",
    "# Drop target, fill in NaNs\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "X_test = test.copy()\n",
    "\n",
    "del train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    78281\n",
       "1     2116\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filter = X_test.copy()\n",
    "X_train_filter = X_train.copy()\n",
    "y_test = X_test_filter['isFraud'].copy()\n",
    "X_test_filter.drop('isFraud', axis = 1, inplace = True)\n",
    "\n",
    "### drop all the NaN columns\n",
    "### dropna() - \n",
    "for column in X_test_filter.columns:\n",
    "    if pd.isnull(X_test_filter[column].unique()[0]):\n",
    "        X_train_filter.drop([column], axis = 1, inplace=True)\n",
    "        X_test_filter.drop([column], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "def label_encoding(X_train, X_test):\n",
    "    for f in X_train.columns:\n",
    "        if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "            X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "            X_test[f] = lbl.transform(list(X_test[f].values))\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filter_, X_test_filter_ = label_encoding(X_train_filter, X_test_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fillna process\n",
    "\"\"\"\n",
    "- mean\n",
    "- mode\n",
    "- 0\n",
    "\"\"\"\n",
    "def fill_na_df(X_train, X_test):\n",
    "    for col_name in X_train.columns: \n",
    "        X_train[col_name].fillna(X_train[col_name].mode()[0], inplace=True)\n",
    "        X_test[col_name].fillna(X_test[col_name].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filter_fill = X_train_filter_.copy()\n",
    "X_test_filter_fill = X_test_filter_.copy()\n",
    "fill_na_df(X_train_filter_fill, X_test_filter_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80397, 225)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_filter_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>...</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_12</th>\n",
       "      <th>id_13</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2987022</th>\n",
       "      <td>86786</td>\n",
       "      <td>50.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1724</td>\n",
       "      <td>583.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>299.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987100</th>\n",
       "      <td>88169</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1</td>\n",
       "      <td>16659</td>\n",
       "      <td>170.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>330.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987104</th>\n",
       "      <td>88208</td>\n",
       "      <td>100.000</td>\n",
       "      <td>1</td>\n",
       "      <td>16659</td>\n",
       "      <td>170.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>330.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987148</th>\n",
       "      <td>88842</td>\n",
       "      <td>30.000</td>\n",
       "      <td>1</td>\n",
       "      <td>14858</td>\n",
       "      <td>558.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>2</td>\n",
       "      <td>325.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987288</th>\n",
       "      <td>90986</td>\n",
       "      <td>155.521</td>\n",
       "      <td>0</td>\n",
       "      <td>16578</td>\n",
       "      <td>545.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>1</td>\n",
       "      <td>299.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 225 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               TransactionDT  TransactionAmt  ProductCD  card1  card2  card3  \\\n",
       "TransactionID                                                                  \n",
       "2987022                86786          50.000          1   1724  583.0  150.0   \n",
       "2987100                88169         100.000          1  16659  170.0  150.0   \n",
       "2987104                88208         100.000          1  16659  170.0  150.0   \n",
       "2987148                88842          30.000          1  14858  558.0  150.0   \n",
       "2987288                90986         155.521          0  16578  545.0  185.0   \n",
       "\n",
       "               card4  card5  card6  addr1  ...     V333  V334  V335  V336  \\\n",
       "TransactionID                              ...                              \n",
       "2987022            4  226.0      1  299.0  ...      0.0   0.0   0.0   0.0   \n",
       "2987100            4  226.0      1  330.0  ...      0.0   0.0   0.0   0.0   \n",
       "2987104            4  226.0      1  330.0  ...    100.0   0.0   0.0   0.0   \n",
       "2987148            4  226.0      2  325.0  ...      0.0   0.0   0.0   0.0   \n",
       "2987288            4  226.0      1  299.0  ...      0.0   0.0   0.0   0.0   \n",
       "\n",
       "               V337  V338  V339  id_01  id_12  id_13  \n",
       "TransactionID                                         \n",
       "2987022         0.0   0.0   0.0  -15.0      1   14.0  \n",
       "2987100         0.0   0.0   0.0    0.0      1   52.0  \n",
       "2987104         0.0   0.0   0.0    0.0      0   52.0  \n",
       "2987148         0.0   0.0   0.0   -5.0      1   49.0  \n",
       "2987288         0.0   0.0   0.0  -20.0      1   49.0  \n",
       "\n",
       "[5 rows x 225 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_filter_fill.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_filter_fill.ProductCD.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "# X_train_filter_pca = clean_dataset(X_train_filter_)\n",
    "# X_test_filter_pca = clean_dataset(X_test_filter_)\n",
    "\n",
    "# pca = PCA(n_components=180)\n",
    "# pca.fit(X_train_filter_fill)\n",
    "# X_train_filter_pca = pca.transform(X_train_filter_fill)\n",
    "# X_test_filter_pca = pca.transform(X_test_filter_fill)\n",
    "X_train_filter_pca = X_train_filter_fill\n",
    "X_test_filter_pca = X_test_filter_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train_filter_fill = reduce_mem_usage(X_train_filter_fill)\n",
    "# X_test_filter_fill = reduce_mem_usage(X_test_filter_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 138.62 MB\n",
      "Memory usage after optimization is: 37.19 MB\n",
      "Decreased by 73.2%\n",
      "Memory usage of dataframe is 17.24 MB\n",
      "Memory usage after optimization is: 4.72 MB\n",
      "Decreased by 72.6%\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_filter_pca = reduce_mem_usage(pd.DataFrame(X_train_filter_pca))\n",
    "X_test_filter_pca = reduce_mem_usage(pd.DataFrame(X_test_filter_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_filter_pca = sc.fit_transform(X_train_filter_pca)\n",
    "# mm=MinMaxScaler()\n",
    "# X_train_filter_pca=mm.fit_transform(X_train_filter_pca)\n",
    "X_test_filter_pca = sc.fit_transform(X_test_filter_pca)\n",
    "\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_train_filter_pca, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FraudNet(\n",
       "  (fc1): Linear(in_features=225, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (fc4): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (fc5): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FraudNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(225, 400)\n",
    "        self.fc2 = nn.Linear(400,50 )\n",
    "        self.fc3 = nn.Linear(50, 30)\n",
    "        self.fc4 = nn.Linear(30, 40)\n",
    "        self.fc5 = nn.Linear(40, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.25)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "net = FraudNet().double()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:516: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:516: UserWarning: Using a target size (torch.Size([97])) that is different to the input size (torch.Size([97, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC 0.9192060899600969\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data_utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train = np.array(X_train_filter_pca)\n",
    "Y_train = np.array(y_train)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train).double()\n",
    "\n",
    "\n",
    "print(X_train.shape[0])\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.002)\n",
    "training_epochs = 10\n",
    "minibatch_size = 100\n",
    "train = data_utils.TensorDataset(X_train, Y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=minibatch_size, shuffle=True)\n",
    "for i in range(training_epochs):\n",
    "    for b, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        y_pred = net(inputs.double())\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        y_pred1=y_pred\n",
    "        labels1=labels\n",
    "\n",
    "#         print('Epochs: {}, batch: {} loss: {}'.format(i, b, loss))\n",
    "#         print('ROC AUC {}'.format(roc_auc_score(labels1.detach().numpy(), y_pred1.detach().numpy())))\n",
    "        #reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "y_pred_total = net(X_train.double())\n",
    "print('ROC AUC {}'.format(roc_auc_score(Y_train.numpy(), y_pred_total.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.27319546e+00 -3.32345743e-01 -1.58006105e-03 ... -3.51091423e-01\n",
      "   4.12477971e-01 -3.10013988e+00]\n",
      " [-1.27290789e+00  1.74159056e-01 -1.58006105e-03 ...  7.11319581e-01\n",
      "   4.12477971e-01  3.05484508e-01]\n",
      " [-1.27289978e+00  1.74159056e-01 -1.58006105e-03 ...  7.11319581e-01\n",
      "  -2.42437190e+00  3.05484508e-01]\n",
      " ...\n",
      " [-1.27203851e+00  1.44042105e+00  9.85958097e-01 ...  3.57182580e-01\n",
      "   4.12477971e-01  3.05484508e-01]\n",
      " [-1.27197551e+00 -4.33646702e-01 -1.58006105e-03 ...  7.11319581e-01\n",
      "   4.12477971e-01  3.05484508e-01]\n",
      " [-1.27167816e+00 -7.67939869e-01  1.97349626e+00 ...  7.11319581e-01\n",
      "  -2.42437190e+00  3.05484508e-01]]\n",
      "(10000, 225)\n",
      "tensor([[-1.2732e+00, -3.3235e-01, -1.5801e-03,  ..., -3.5109e-01,\n",
      "          4.1248e-01, -3.1001e+00],\n",
      "        [-1.2729e+00,  1.7416e-01, -1.5801e-03,  ...,  7.1132e-01,\n",
      "          4.1248e-01,  3.0548e-01],\n",
      "        [-1.2729e+00,  1.7416e-01, -1.5801e-03,  ...,  7.1132e-01,\n",
      "         -2.4244e+00,  3.0548e-01],\n",
      "        ...,\n",
      "        [-1.2720e+00,  1.4404e+00,  9.8596e-01,  ...,  3.5718e-01,\n",
      "          4.1248e-01,  3.0548e-01],\n",
      "        [-1.2720e+00, -4.3365e-01, -1.5801e-03,  ...,  7.1132e-01,\n",
      "          4.1248e-01,  3.0548e-01],\n",
      "        [-1.2717e+00, -7.6794e-01,  1.9735e+00,  ...,  7.1132e-01,\n",
      "         -2.4244e+00,  3.0548e-01]], dtype=torch.float64)\n",
      "torch.Size([10000, 225])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.18012146e-04]\n",
      " [4.07336017e-05]\n",
      " [1.53952117e-04]\n",
      " [9.42048440e-06]\n",
      " [1.16704191e-02]\n",
      " [8.84874237e-02]\n",
      " [4.07693578e-02]\n",
      " [7.93544456e-02]\n",
      " [5.12722736e-04]\n",
      " [1.22778391e-06]]\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(X_test_filter_pca[:10])\n",
    "print(X_test_filter_pca.shape)\n",
    "X_test = np.array(X_test_filter_pca)\n",
    "# Y_test= np.array(Y_test2)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "print(X_test[:10])\n",
    "print(X_test.shape)\n",
    "# Y_test = torch.from_numpy(Y_test).double()\n",
    "# test = data_utils.TensorDataset(X_test, Y_test)\n",
    "# test_loader = data_utils.DataLoader(test, batch_size=minibatch_size, shuffle=True)\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in test_loader:\n",
    "#         inputs, labels = data\n",
    "#         outputs = net(inputs.double())\n",
    "# #         print('ROC AUC {}'.format(roc_auc_score(labels.detach().numpy(), outputs.detach().numpy())))\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted.double() == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the network on the {} inputs: {}'.format(\n",
    "#     X_test.shape[0], 100 * correct/total))\n",
    "# y_test_pred = net(X_test.double())\n",
    "# print('ROC AUC {}'.format(roc_auc_score(Y_test.numpy(), y_test_pred.detach().numpy())))\n",
    "\n",
    "y_text_pred=net(X_test.double())\n",
    "sample_submission['isFraud'] = y_text_pred.detach().numpy()\n",
    "print(y_text_pred.detach().numpy()[:10])\n",
    "print(y_text_pred.detach().numpy().shape)\n",
    "sample_submission.to_csv('newsample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
